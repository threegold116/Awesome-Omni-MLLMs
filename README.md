<div align="center">
<h2>
From Specific-MLLMs to Omni-MLLMs: 
A Survey on MLLMs Aligned with Multi-modalities
</h2>
</div>

<div align="center">
<b>Shixin Jiang</b><sup>1‚àó</sup>,
<b>Jiafeng Liang</b><sup>1</sup>,
<b>Jiyuan Wang</b><sup>1</sup>,
<b>Xuan Dong</b><sup>1</sup>,
<b>Heng Chang</b><sup>2</sup>,
<b>Weijiang Yu</b><sup>2</sup>,
<b>Jinhua Du</b><sup>2</sup>,
<b>Ming Liu</b><sup>1‚Ä†</sup>,
<b>Bing Qin</b><sup>1</sup>
</div>

<div align="center">
<sup>1</sup>Harbin Institute of Technology, Harbin, China
</div>
<div align="center">
<sup>2</sup>Huawei Inc., Shenzhen, China
</div>

<br />

<div align="center">
    <!-- <a href="https://doi.org/10.48550/arXiv.2309.15402"><img src="https://img.shields.io/badge/ACL-2024-b31b1b.svg" alt="Paper"></a> -->
    <a href="https://doi.org/10.48550/arXiv.2412.11694"><img src="https://img.shields.io/badge/arXiv-2412.11694-b31b1b.svg" alt="Paper"></a>
    <a href="https://github.com/threegold116/Omni-MLLM"><img src="https://img.shields.io/github/last-commit/threegold116/Omni-MLLM?color=blue" alt="Github"></a>
    <a href="https://github.com/threegold116/Omni-MLLM/blob/main/LICENSE"> <img alt="License" src="https://img.shields.io/github/license/zchuz/CoT-Reasoning-Survey?color=green"> </a>
</div>

This repository contains the resources for **arxiv** paper **_From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities_**

![taxonomy](figure/taxonomy.png)

For more details, please refer to the paper: [From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities](https://doi.org/10.48550/arXiv.2412.11694).



## üéâ Updates
- 2025/02/28 We have updated 23 papers in the reading list.
- 2025/02/20 We have updated 55 papers first in the reading list.
- 2025/02/15 The second version of our paper has been released, check it on [arxiv](https://doi.org/10.48550/arXiv.2412.11694).
- 2024/12/16 The first version of our paper is available on [arxiv](https://doi.org/10.48550/arXiv.2412.11694).
- 2024/09/22 We created this reading list repository.

<!-- We use the üí° icon to identify articles that have been added since the last version of the paper -->

This reading list will be updated periodically, and if you have any suggestions or find some we missed, feel free to contact us! You can submit an issue or send an email (sxjiang@ir.hit.edu.cn).

# Awesome Omni-MLLMs
Omni-MLLM: The MLLMs which can handle more than 2 extra-linguistic modalities.
## Multi-branch Continuous Omni-MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Demo   |Cross(Omni)-modal<br>Understanding|Cross(Omni)-modal<br>Generation|
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
|  ![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star) <br>[**Qwen2.5-Omni Technical Report**](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf) <br> | blog | 2025-03-27 | [Github](https://github.com/QwenLM/Qwen2.5-Omni/)   | [Demo](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo) | ‚úÖ | ‚úÖ|
|  ![Star](https://img.shields.io/github/stars/kyutai-labs/moshivis.svg?style=social&label=Star) <br>[**Vision-Speech Models: Teaching Speech Models to Converse about Images**](https://arxiv.org/pdf/2503.15633) <br> | arXiv | 2025-03-20 | [Github](https://github.com/kyutai-labs/moshivis/)   | - | ‚úÖ | ‚úÖ|
|  ![Star](https://img.shields.io/github/stars/HumanMLLM/ViSpeak.svg?style=social&label=Star) <br> [**ViSpeak: Visual Instruction Feedback in Streaming Videos**](https://arxiv.org/pdf/2503.12769) <br> | arXiv | 2025-03-17 | [Github](https://github.com/HumanMLLM/ViSpeak/)  | - | ‚úÖ | ‚úÖ|
|  ![Star](https://img.shields.io/github/stars/GeWu-Lab/Crab.svg?style=social&label=Star) <br> [**Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation**](https://arxiv.org/pdf/2503.13068v1) <br> | CVPR | 2025-03-17 | [Github](https://github.com/GeWu-Lab/Crab/)  | - | ‚úÖ | ‚ùå|
|  ![Star](https://img.shields.io/github/stars/JeongHun0716/MMS-LLaMA.svg?style=social&label=Star) <br> [**MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens**](https://arxiv.org/pdf/2503.11315) <br> | arXiv | 2025-03-14 | [Github](https://github.com/JeongHun0716/MMS-LLaMA/)  | - | ‚úÖ | ‚ùå|
|   [**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**](https://arxiv.org/pdf/2503.08481) <br> | arXiv | 2025-03-11 | - | - | ‚úÖ | ‚ùå|
| [**Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs**](https://arxiv.org/pdf/2503.06362) <br> | arXiv | 2025-03-09 | - | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/HumanMLLM/R1-Omni.svg?style=social&label=Star) <br>[**R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning**](https://arxiv.org/pdf/2503.05379) <br> | arXiv | 2025-03-07 | [Github](https://github.com/HumanMLLM/R1-Omni/) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/microsoft/PhiCookBook.svg?style=social&label=Star) <br> [**Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs**](https://arxiv.org/pdf/2503.01743) <br> | arXiv | 2025-03-03 | [Github](https://github.com/microsoft/PhiCookBook/) | - | ‚úÖ | ‚ùå|
| [**Nexus-O: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision**](https://arxiv.org/pdf/2503.01879) <br> | arXiv | 2025-02-26 | | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/infinigence/Infini-Megrez-Omni.svg?style=social&label=Star) <br> [**Megrez-Omni Technical Report**](https://www.arxiv.org/pdf/2502.15803) <br> | arXiv | 2025-02-19 | [Github](https://github.com/infinigence/Infini-Megrez-Omni) | - | ‚úÖ | ‚ùå|
| [**Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-based Benchmark**](https://arxiv.org/pdf/2502.04976) <br> | WWW | 2025-02-07 | - | - | ‚úÖ | ‚úÖ|
| [**M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance**](https://arxiv.org/pdf/2502.18778) <br> | arXiv | 2025-02-06 | - | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/Ola-Omni/Ola.svg?style=social&label=Star) <br> [**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**](https://arxiv.org/pdf/2502.04328) <br> | arXiv | 2025-02-06 | [Github](https://github.com/Ola-Omni/Ola) | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Omni-1.5.svg?style=social&label=Star) <br> [**Baichuan-Omni-1.5 Technical Report**](https://arxiv.org/abs/2501.15368) <br> | arXiv | 2025-01-26 | [Github](https://github.com/baichuan-inc/Baichuan-Omni-1.5) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/HumanMLLM/HumanOmni.svg?style=social&label=Star) <br> [**HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding**](https://arxiv.org/pdf/2501.15111) <br> | arXiv | 2025-01-25 | [Github](https://github.com/HumanMLLM/HumanOmni) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o.svg?style=social&label=Star) <br> [**MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone**](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9) <br> | blog | 2025-01-24 | [Github](https://github.com/OpenBMB/MiniCPM-o) | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/HumanMLLM/Omni-Emotion.svg?style=social&label=Star) <br> [**Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis**](https://arxiv.org/pdf/2501.04561) <br> | arXiv | 2025-01-16 | [Github](https://github.com/HumanMLLM/Omni-Emotion) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/RainBowLuoCS/OpenOmni.svg?style=social&label=Star) <br> [**OpenOmni: A Fully Open-Source Omni Large Language Model with Real-time Self-Aware Emotional Speech Synthesis**](https://arxiv.org/pdf/2501.04561) <br> | arXiv | 2025-01-09 | [Github](https://github.com/RainBowLuoCS/OpenOmni) | - | ‚úÖ | ‚úÖ|
|  [**Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor**](https://arxiv.org/pdf/2412.17572) <br> | arXiv | 2024-12-23 | - | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br> [**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**](https://arxiv.org/pdf/2501.01957) <br> | arXiv | 2024-12-20 | [Github](https://github.com/VITA-MLLM/VITA) | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star) <br> [**Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition**](https://arxiv.org/pdf/2412.09501) <br> | arXiv | 2024-12-12 | [Github](https://github.com/dvlab-research/Lyra) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/shansongliu/MuMu-LLaMA.svg?style=social&label=Star) <br> [**MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models**](https://arxiv.org/pdf/2412.06660) <br> | arXiv | 2024-12-09 | [Github](https://github.com/shansongliu/MuMu-LLaMA) | - | ‚úÖ | ‚úÖ|
| [**LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**](https://arxiv.org/pdf/2411.19772) <br> | CVPR | 2024-11-29 | -| - | ‚úÖ | ‚ùå|
| [**Spider: Any-to-Many Multimodal LLM**](https://arxiv.org/pdf/2411.09439) <br> | arXiv | 2024-11-14 | - | - | ‚úÖ | ‚úÖ|
| [**Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model**](https://arxiv.org/pdf/2411.05903) <br> | arXiv | 2024-11-08 |- | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/CAD-MLLM/CAD-MLLM.svg?style=social&label=Star) <br> [**CAD-MLLM: Unifying Multimodality-ConditionedCAD Generation With MLLM**](https://arxiv.org/pdf/2411.04954) <br> | arXiv | 2024-11-07 | [Github](https://github.com/CAD-MLLM/CAD-MLLM) | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/JiazuoYu/PathWeave.svg?style=social&label=Star) <br> [**LLMs Can Evolve Continually on Modality for X-Modal Reasoning**](https://arxiv.org/pdf/2410.20178) <br> | NeurIPS | 2024-10-26 | [Github](https://github.com/JiazuoYu/PathWeave) | - | ‚úÖ | ‚ùå|
|  [**OMCAT Omni Context Aware Transformer**](https://arxiv.org/pdf/2410.12109) <br> | arXiv | 2024-10-15 | - | - |
| ![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni2.svg?style=social&label=Star) <br> [**Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities**](https://arxiv.org/pdf/2410.11190) <br> | NeurIPS | 2024-10-15 | [Github](https://github.com/gpt-omni/mini-omni2) | - | ‚úÖ | ‚ùå|
|  [**Aligned Better, Listen Better For Audio-Visual Large Language Models**](https://openreview.net/pdf?id=1SYUKPeM12) <br> | ICLR | 2024-10-13 | - | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/westlake-baichuan-mllm/bc-omni.svg?style=social&label=Star) <br> [**Baichuan-Omni Technical Report**](https://arxiv.org/pdf/2410.08565) <br> | arXiv | 2024-10-11 | [Github](https://github.com/westlake-baichuan-mllm/bc-omni) | - | ‚úÖ | ‚ùå|
| [**Efficient Generative Multimodal Integration (EGMI): Enabling Audio Generation from Text-Image Pairs through Alignment with Large Language Models**](https://openreview.net/pdf?id=5fWY2ZlsKj) <br> | NeurIPS<br>Workshop | 2024-10-11 | - | - | ‚úÖ | ‚úÖ|
| [**Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization**](https://arxiv.org/pdf/2410.06682) <br> | arXiv | 2024-10-09 | - | - | ‚úÖ | ‚ùå|
|   [**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**](https://arxiv.org/pdf/2409.18042) <br> | CVPR | 2024-09-26 | -| - | ‚úÖ | ‚úÖ|
|  [**Large Language Models Are Strong Audio-Visual Speech Recognition Learners**](https://arxiv.org/abs/2409.12319) <br> | ICASSP | 2024-09-18 | -| - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br> [**VITA: Towards Open-Source Interactive Omni Multimodal LLM**](https://arxiv.org/pdf/2408.05211) <br> | arXiv | 2024-08-05 | [Github](https://github.com/VITA-MLLM/VITA) | - | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/lzw-lzw/UnifiedMLLM.svg?style=social&label=Star) <br> [**UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model**](https://arxiv.org/pdf/2408.02503) <br> | arXiv | 2024-08-03 | [Github](https://github.com/lzw-lzw/UnifiedMLLM) | - | ‚úÖ | ‚úÖ|
| [**InternOmni: Extending InternVL with Audio Modality**](https://internvl.github.io/blog/2024-07-27-InternOmni/) <br> | blog | 2024-07-31 | - | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/schowdhury671/meerkat.svg?style=social&label=Star) <br> [**Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time**](https://arxiv.org/pdf/2407.01851) <br> | arXiv | 2024-07-01 | [Github](https://github.com/schowdhury671/meerkat) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&label=Star) <br> [**video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models**](https://arxiv.org/pdf/2406.15704) <br> | arXiv | 2024-06-22 | [Github](https://github.com/bytedance/SALMONN/) | - | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/ZebangCheng/Emotion-LLaMA.svg?style=social&label=Star) <br> [**Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning**](https://arxiv.org/pdf/2406.11161) <br> | NIPS  | 2024-06-17 | [Github](https://github.com/ZebangCheng/Emotion-LLaMA) | [Demo](https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA) | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star) <br> [**VideoLLaMA 2 Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**](https://arxiv.org/pdf/2406.07476) <br> | arXiv  | 2024-06-11 | [Github](https://github.com/DAMO-NLP-SG/VideoLLaMA2) | [Demo](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2-AV) | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/scofield7419/MUIE-REAMO.svg?style=social&label=Star) <br> [**Recognizing Everything from All Modalities at Once: Grounded Multimodal Universal Information Extraction**](https://aclanthology.org/2024.findings-acl.863.pdf) <br> | ACL  | 2024-06-06 | [Github](https://github.com/scofield7419/MUIE-REAMO) |-| ‚úÖ | ‚ùå|
| [**X-VILA: Cross-Modality Alignment for Large Language Model**](https://arxiv.org/pdf/2405.19335) <br> | arXiv  | 2024-05-29 | - |- | ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.svg?style=social&label=Star) <br> [**Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts**](https://arxiv.org/pdf/2405.19335) <br> | TPAMI  | 2024-05-18 | [Github](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs) | Local<br>Demo | ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/Rainlt/QaP.svg?style=social&label=Star) <br> [**Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf) <br> | CVPR  | 2024-05-06 | [Github](https://github.com/Rainlt/QaP/) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/DCDmllm/WorldGPT.svg?style=social&label=Star) <br> [**WorldGPT: Empowering LLM as Multimodal World Model**](https://arxiv.org/pdf/2404.18202) <br> | AAAI  | 2024-04-28 | [Github](https://github.com/DCDmllm/WorldGPT) |-| ‚úÖ | ‚úÖ|
|  [**Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding**](https://arxiv.org/pdf/2403.16276) <br> | arXiv  | 2024-03-24 | - |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/rikeilong/Bay-CAT.svg?style=social&label=Star) <br> [**CAT : Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios**](https://arxiv.org/pdf/2403.04640) <br> | ECCV  | 2024-03-04 | [Github](https://github.com/rikeilong/Bay-CAT) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/THUNLP-MT/ModelCompose.svg?style=social&label=Star) <br> [**Model Composition for Multimodal Large Language Models**](https://arxiv.org/pdf/2402.12750) <br> | ACL  | 2024-02-18 | [Github](https://github.com/THUNLP-MT/ModelCompose) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/lzw-lzw/GroundingGPT.svg?style=social&label=Star) <br> [**GroundingGPT: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/pdf/2401.06071) <br> | ACL  | 2024-02-11 | [Github](https://github.com/lzw-lzw/GroundingGPT) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/Yui010206/CREMA.svg?style=social&label=Star) <br> [**CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion**](https://arxiv.org/pdf/2402.05889) <br> | ICLR  | 2024-02-08 | [Github](https://github.com/Yui010206/CREMA) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/UMass-Foundation-Model/MultiPLY.svg?style=social&label=Star) <br> [**MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World**](https://arxiv.org/pdf/2401.08577) <br> | CVPR  | 2024-01-16 | [Github](https://github.com/UMass-Foundation-Model/MultiPLY) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/xinke-wang/ModaVerse.svg?style=social&label=Star) <br> [**ModaVerse: Efficiently Transforming Modalities with LLMs**](https://arxiv.org/pdf/2401.06395) <br> | CVPR  | 2024-01-12 | [Github](https://github.com/xinke-wang/ModaVerse) |-| ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&label=Star) <br> [**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Actions**](https://arxiv.org/pdf/2312.17172) <br> | CVPR  | 2023-12-28 | [Github](https://github.com/allenai/unified-io-2) |-| ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/OpenM3D/M3DBench.svg?style=social&label=Star) <br> [**M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07509.pdf) <br> | ECCV  | 2023-12-17 | [Github](https://github.com/OpenM3D/M3DBench) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM.svg?style=social&label=Star) <br> [**DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving**](https://arxiv.org/pdf/2312.09245) <br> | arXiv  | 2023-12-14 | [Github](https://github.com/OpenGVLab/DriveMLM) |-| ‚úÖ | ‚ùå|
|  [**Audio-Visual LLM for Video Understanding**](https://arxiv.org/pdf/2312.06720) <br> | arXiv  | 2023-12-11 |- |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning**](https://arxiv.org/pdf/2311.18799) <br> | ECCV  | 2023-11-30 | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LAMM.svg?style=social&label=Star) <br> [**Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE**](https://arxiv.org/pdf/2311.02684) <br> | ICLR  | 2023-11-05 | [Github](https://github.com/OpenGVLab/LAMM) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/BriansIDP/AudioVisualLLM.svg?style=social&label=Star) <br> [**Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models**](https://arxiv.org/pdf/2310.05863) <br> | arXiv  | 2023-10-09 | [Github](https://github.com/BriansIDP/AudioVisualLLM) |[Demo](https://881c5a6a6db84b1a2f.gradio.live/)|‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/kyegomez/AnyMAL.svg?style=social&label=Star) <br> [**AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model**](https://aclanthology.org/2024.emnlp-industry.98.pdf) <br> | EMNLP<br>industry  | 2023-09-27 | [Github](https://github.com/kyegomez/AnyMAL) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star) <br> [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519) <br> | ICML  | 2023-09-11 | [Github](https://github.com/NExT-GPT/NExT-GPT) |-| ‚úÖ | ‚úÖ|
| ![Star](https://img.shields.io/github/stars/magic-research/bubogpt.svg?style=social&label=Star) <br> [**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**](https://arxiv.org/pdf/2309.05519) <br> | arXiv  | 2023-07-17 | [Github](https://github.com/magic-research/bubogpt) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM.svg?style=social&label=Star) <br> [**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://arxiv.org/pdf/2306.09093) <br> | arXiv  | 2023-06-15 | [Github](https://github.com/lyuchenyang/Macaw-LLM) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LAMM.svg?style=social&label=Star) <br> [**LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark**](https://proceedings.neurips.cc/paper_files/paper/2023/file/548a41b9cac6f50dccf7e63e9e1b1b9b-Paper-Datasets_and_Benchmarks.pdf) <br> | NerIPS  | 2023-06-13 | [Github](https://github.com/OpenGVLab/LAMM) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://aclanthology.org/2023.emnlp-demo.49.pdf) <br> | EMNLP<br>demo  | 2023-06-05 | [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/joez17/ChatBridge.svg?style=social&label=Star) <br> [**ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst**](https://arxiv.org/pdf/2305.16103) <br> | arXiv | 2023-05-25 | [Github](https://github.com/joez17/ChatBridge) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/phellonchen/X-LLM.svg?style=social&label=Star) <br> [**X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages**](https://arxiv.org/pdf/2305.04160) <br> | arXiv | 2023-05-07 | [Github](https://github.com/phellonchen/X-LLM) |-| ‚úÖ | ‚ùå|
| ![Star](https://img.shields.io/github/stars/mshukor/eP-ALM.svg?style=social&label=Star) <br> [**eP-ALM: Efficient Perceptual Augmentation of Language Models**](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf) <br> | ICCV | 2023-03-20 | [Github](https://github.com/mshukor/eP-ALM) |-| ‚úÖ | ‚ùå|
<!-- | ![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star) <br> [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://arxiv.org/pdf/2311.18775) <br> | CVPR  | 2025-02-07 | [Github](https://github.com/microsoft/i-Code/tree/main/CoDi-2) |-| -->


## Uni-branch Continuous Omni-MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/JiazuoYu/PathWeave.svg?style=social&label=Star) <br> [**LLMs Can Evolve Continually on Modality for X-Modal Reasoning**](https://openreview.net/pdf?id=5fWY2ZlsKj) <br> | NeurIPS | 2024-10-26 | [Github](https://github.com/JiazuoYu/PathWeave) | - |
| [**Efficient Generative Multimodal Integration (EGMI): Enabling Audio Generation from Text-Image Pairs through Alignment with Large Language Models**](https://openreview.net/pdf?id=5fWY2ZlsKj) <br> | NeurIPS<br>Workshop | 2024-10-11 | - | - |
| ![Star](https://img.shields.io/github/stars/scofield7419/EmpathyEar.svg?style=social&label=Star) <br> [**EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot**](https://arxiv.org/pdf/2406.15177) <br> | ACL<br>Demo | 2024-06-21 | [Github](https://github.com/scofield7419/EmpathyEar) | - |
| ![Star](https://img.shields.io/github/stars/DCDmllm/WorldGPT.svg?style=social&label=Star) <br> [**WorldGPT: Empowering LLM as Multimodal World Model**](https://arxiv.org/pdf/2404.18202) <br> | MM | 2024-04-28 | [Github](https://github.com/DCDmllm/WorldGPT) | - |
| ![Star](https://img.shields.io/github/stars/Max-Fu/tvl.svg?style=social&label=Star) <br> [**A Touch, Vision, and Language Dataset for Multimodal Alignment**](https://openreview.net/pdf?id=tFEOOH9eH0) <br> | ICML | 2024-02-20 | [Github](https://github.com/Max-Fu/tvl) | - |
| ![Star](https://img.shields.io/github/stars/Yui010206/CREMA.svg?style=social&label=Star) <br> [**CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion**](https://arxiv.org/pdf/2402.05889) <br> | ICLR | 2024-02-08 | [Github](https://github.com/Yui010206/CREMA) | - |
| ![Star](https://img.shields.io/github/stars/csuhan/OneLLM.svg?style=social&label=Star) <br> [**OneLLM: One Framework to Align All Modalities with Language**](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf) <br> | CVPR | 2023-11-30 | [Github](https://github.com/csuhan/OneLLM) | - |
| ![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star) <br> [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.pdf) <br> | CVPR | 2023-11-30 | [Github](https://github.com/microsoft/i-Code/tree/main/CoDi-2) | - |
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**ImageBind-LLM: Multi-modality Instruction Tuning**](https://arxiv.org/pdf/2309.03905) <br> | arXiv | 2023-09-11 | [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | - |
| ![Star](https://img.shields.io/github/stars/yxuansu/PandaGPT.svg?style=social&label=Star) <br> [**PandaGPT: One Model To Instruction-Follow Them All**](https://aclanthology.org/2023.tllm-1.2.pdf) <br> | tllm | 2023-05-25 | [Github](https://github.com/yxuansu/PandaGPT) | - |


## Diserect Omni-MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|  [**SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters**](https://arxiv.org/pdf/2412.00174) <br> | Arxiv | 2024-12-29 | - | - |
| [**LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis**](https://arxiv.org/pdf/2410.10851) <br> | Arxiv | 2024-10-06 |- | - |
| ![Star](https://img.shields.io/github/stars/MIO-Team/MIO.svg?style=social&label=Star) <br> [**MIO: A Foundation Model on Multimodal Tokens**](https://arxiv.org/pdf/2409.17692) <br> | Arxiv | 2024-09-26 |  [Github](https://github.com/MIO-Team/MIO) | - |
| [**OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving**](https://arxiv.org/pdf/2409.03272v1) <br> | Arxiv | 2024-09-05 | - | - |
| ![Star](https://img.shields.io/github/stars/luomingshuang/M3GPT.svg?style=social&label=Star) <br> [**M3GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation**](https://openreview.net/pdf?id=ODbTlAs0Oj) <br> | NeurIPS | 2024-02-19 | [Github](https://github.com/luomingshuang/M3GPT) | - |
| ![Star](https://img.shields.io/github/stars/OpenMOSS/AnyGPT.svg?style=social&label=Star) <br> [**AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**](https://aclanthology.org/2024.acl-long.521.pdf) <br> | ACL | 2024-02-19 | [Github](https://github.com/OpenMOSS/AnyGPT) | - |
| [**TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models**](https://arxiv.org/pdf/2311.04589) <br> | Arxiv | 2023-11-08 | - | - |

## Hybrid Omni-MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons**](https://arxiv.org/pdf/2412.08442) <br> | Arxiv | 2024-12-11 |- | - |
| [**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**](https://arxiv.org/pdf/2409.18042) <br> | Arxiv | 2024-09-26 |- | - |
| [**Grounding Multimodal Large Language Models in Actions**](https://arxiv.org/pdf/2406.07904) <br> | NeurIPS | 2024-06-12 |- | - |
| ![Star](https://img.shields.io/github/stars/embodied-generalist/embodied-generalist.svg?style=social&label=Star) <br> [**An Embodied Generalist Agent in 3D World**](https://arxiv.org/pdf/2311.12871) <br> | ICML | 2023-11-18 | [Github](https://github.com/embodied-generalist/embodied-generalist) | - |

# Cross-Modality Benchmark
## Comprehensive Understanding Benchmark
|  Title  |   Name  |   Date   |   Download   |   Modality   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**ACVUBench: Audio-Centric Video Understanding Benchmark**](https://arxiv.org/pdf/2503.19951) <br> | DAVE | 2025-03-25 | -- | Video,Audio,<br>Text |
| [**DAVE : Diagnostic benchmark for Audio Visual Evaluation**](https://arxiv.org/pdf/2503.09321) <br> | DAVE | 2025-03-12 | -- | Video,Audio,<br>Text |
| [**WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs**](https://arxiv.org/abs/2502.04326) <br> | WorldSense | 2025-02-06 |[Huggingface](https://huggingface.co/datasets/honglyhly/WorldSense) | Image,Video,<br>Audio,Text |
| [**AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs**](https://arxiv.org/abs/2501.02135) <br> | AVTrustBench | 2025-01-03 |- | Image,Video,<br>Audio,Text |
| [**Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback**](https://arxiv.org/pdf/2412.15838) <br> | AMU | 2024-12-20 |- | Image,Video,<br>Audio,Text |
| [**OmniBench: Towards The Future of Universal Omni-Language Models**](https://arxiv.org/pdf/2409.15272) <br> | OmniBench | 2024-12-11 |[Huggingface](https://huggingface.co/datasets/m-a-p/OmniBench) | Image,Audio,<br>Text |
| [**AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?**](https://arxiv.org/abs/2412.02611) <br> | AV-Odyssey<br>Bench | 2024-12-03 |[Huggingface](https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench)  | Image,Video,<br>Audio,Text |
| [**AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models**](https://openreview.net/pdf?id=jTEKTdI3K9) <br> | AVHBench | 2024-10-23 |[Google Driver](https://drive.google.com/file/d/10-Qp8zxA3ITT-ileEnCgJkf5Nzx1wry7/view)<br>[Google Driver](https://drive.google.com/file/d/1KcYDAv9lLy3hsx5rWdfRqMFV2NYcZ94W/view) | Image,Video,<br>Audio,Text |
| [**LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**](https://arxiv.org/abs/2411.19772) <br> | LongVALE | 2024-10-16 |- | Image,Video,<br>Audio,Text |
| [**The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio**](https://arxiv.org/abs/2410.12787) <br> | Curse | 2024-10-16 |[Huggingface](https://huggingface.co/datasets/DAMO-NLP-SG/CMM)  | Image,Video,<br>Audio,Text |
| [**OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities**](https://arxiv.org/abs/2410.12219) <br> | OmnixR | 2024-10-16 |[Huggingface](https://huggingface.co/datasets/Lichang-Chen/omnixR-data) | Image,Video,<br>Audio,Text |
| [**ShareGPT-4o: Comprehensive Multimodal Annotations With GPT-4o**](https://sharegpt4o.github.io/) <br> | ShareGPT-4o | 2024-08-06 |[Huggingface](https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o/tree/main/audio_benchmark) | Image,Video,<br>Audio,Text |
| [**video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models**](https://arxiv.org/pdf/2406.15704) <br> | SAVE    | 2024-06-22 |[Huggingface](https://huggingface.co/datasets/tsinghua-ee/SAVEBench) | Image,Video,<br>Audio,Text |
| [**Model Composition for Multimodal Large Language Models**](https://arxiv.org/abs/2402.12750) <br> | MCUB   | 2024-02-20 |- | Image,Audio,<br>3D,Text |
| [**A Touch, Vision, and Language Dataset for Multimodal Alignment**](https://arxiv.org/abs/2402.13232) <br> | TVL<br>Benchmark   | 2024-02-20 |- | Image,Touch,<br>Text |
| [**X-InstructBLIP: A Framework for Aligning Image, 3D, Audio, Video to LLMs and its Emergent Cross-Modal Reasoning**](https://arxiv.org/pdf/2311.18799) <br> | DisCRn  | 2023-11-30 |[Github](https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip) | Image,Audio,<br>3D,Text |


## Down-Streaming Understanding Benchmark
|  Title  |   Name  |   Date   |   Download   |   Modality   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**Recognizing Everything from All Modalities at Once: Grounded Multimodal Universal Information Extraction**](https://arxiv.org/pdf/2406.03701) <br> |  MUIE | 2024-06-06 |- | Image,Video,<br>Audio,Text |
| [**VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset**](https://arxiv.org/pdf/2304.08345) <br> |  VALOR-32K | 2023-04-17 |[BaiduDisk](https://pan.baidu.com/share/init?surl=aHWCwUOX1lJi0lSsmJb6Tw&pwd=e3ve)<br>[BaiduDisk](https://pan.baidu.com/share/init?surl=Hq6VvAdKlPcq7j737ZXdbA&pwd=qiw1) | Video,Audio,<br>Text |
| [**Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline**](https://arxiv.org/pdf/2303.12930) <br> |  UnAV-100<br>test | 2023-03-22 |- | Video,Audio,<br>Text |
| [**AVQA: A Dataset for Audio-Visual Question Answering on Videos**](https://dl.acm.org/doi/10.1145/3503161.3548291) <br> |  AVQA | 2022-10-10 |[Dataset](https://mn.cs.tsinghua.edu.cn/avqa/) | Video,Audio,<br>Text |
| [**Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing**](https://arxiv.org/pdf/2007.10558) <br> |  LLP | 2022-07-21 |--| Video,Audio,<br>Text |
| [**Learning to Answer Questions in Dynamic Audio-Visual Scenarios**](https://arxiv.org/pdf/2203.14072) <br> |  Music-AVQA | 2022-05-26 |[Google Driver](https://drive.google.com/drive/folders/1WAryZZE0srLIZG8VHl22uZ3tpbGHtsrQ)<br>[Github](https://github.com/GeWu-Lab/MUSIC-AVQA/tree/main/data/json) | Video,Audio,<br>Text |
| [**Pano-AVQA: Grounded Audio-Visual Question Answering on 360 Videos**](https://arxiv.org/pdf/2110.05122) <br> |  Pano-AVQA | 2021-10-11 |-- | Video,Audio,<br>Text |
| [**VGGSound: A Large-scale Audio-Visual Dataset**](https://arxiv.org/pdf/2004.14368) <br> |  VGGSound | 2020-04-29 |[Huggingface](https://huggingface.co/datasets/Loie/VGGSound) | Video,Audio,<br>Text |
| [**Audio Visual Scene-Aware Dialog**](https://arxiv.org/pdf/1901.09107) <br> |  AVSD | 2019-01-25 |[Dataset](https://video-dialog.com/) | Video,Audio,<br>Text |