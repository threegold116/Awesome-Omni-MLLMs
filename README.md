<div align="center">
<h2>
From Specific-MLLMs to Omni-MLLMs: 
A Survey on MLLMs Aligned with Multi-modalities
</h2>
</div>

<div align="center">
<b>Shixin Jiang</b><sup>1âˆ—</sup>,
<b>Jiafeng Liang</b><sup>1</sup>,
<b>Jiyuan Wang</b><sup>1</sup>,
<b>Xuan Dong</b><sup>1</sup>,
<b>Heng Chang</b><sup>2</sup>,
<b>Weijiang Yu</b><sup>2</sup>,
<b>Jinhua Du</b><sup>2</sup>,
<b>Ming Liu</b><sup>1â€ </sup>,
<b>Bing Qin</b><sup>1</sup>
</div>

<div align="center">
<sup>1</sup>Harbin Institute of Technology, Harbin, China
</div>
<div align="center">
<sup>2</sup>Huawei Inc., Shenzhen, China
</div>

<br />

<div align="center">
    <!-- <a href="https://doi.org/10.48550/arXiv.2309.15402"><img src="https://img.shields.io/badge/ACL-2024-b31b1b.svg" alt="Paper"></a> -->
    <a href="https://doi.org/10.48550/arXiv.2412.11694"><img src="https://img.shields.io/badge/arXiv-2412.11694-b31b1b.svg" alt="Paper"></a>
    <a href="https://github.com/threegold116/Omni-MLLM"><img src="https://img.shields.io/github/last-commit/threegold116/Omni-MLLM?color=blue" alt="Github"></a>
    <a href="https://github.com/threegold116/Omni-MLLM/blob/main/LICENSE"> <img alt="License" src="https://img.shields.io/github/license/zchuz/CoT-Reasoning-Survey?color=green"> </a>
</div>

This repository contains the resources for **arxiv** paper **_From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities_**

![taxonomy](figure/taxonomy.png)

For more details, please refer to the paper: [From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities](https://doi.org/10.48550/arXiv.2412.11694).



## ðŸŽ‰ Updates

- 2025/02/20 We have updated 55 papers first in the reading list.
- 2025/02/15 The second version of our paper has been released, check it on [arxiv](https://doi.org/10.48550/arXiv.2412.11694).
- 2024/12/16 The first version of our paper is available on [arxiv](https://doi.org/10.48550/arXiv.2412.11694).
- 2024/09/22 We created this reading list repository.

We use the ðŸ’¡ icon to identify articles that have been added since the last version of the paper

This reading list will be updated periodically, and if you have any suggestions or find some we missed, feel free to contact us! You can submit an issue or send an email (zchu@ir.hit.edu.cn).


# Awesome Omni-MLLMs
Omni-MLLM: The MLLMs which can handle more than 2 extra-linguistic modalities.
## Multi-branch Omni-MLLMs
|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/Ola-Omni/Ola.svg?style=social&label=Star) <br> [**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**](https://arxiv.org/abs/2502.04328) <br> | blog | 2025-02-06 | [Github](https://github.com/Ola-Omni/Ola) | - |
| ![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o.svg?style=social&label=Star) <br> [**MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone**](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9) <br> | blog | 2025-01-24 | [Github](https://github.com/OpenBMB/MiniCPM-o) | - |
| ![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Omni-1.5.svg?style=social&label=Star) <br> [**Baichuan-Omni-1.5 Technical Report**](https://arxiv.org/abs/2501.15368) <br> | arXiv | 2025-01-26 | [Github](https://github.com/baichuan-inc/Baichuan-Omni-1.5) | - |
| ![Star](https://img.shields.io/github/stars/HumanMLLM/Omni-Emotion.svg?style=social&label=Star) <br> [**Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis**](https://arxiv.org/pdf/2501.04561) <br> | arXiv | 2025-01-16 | [Github](https://github.com/HumanMLLM/Omni-Emotion) | - |
| ![Star](https://img.shields.io/github/stars/RainBowLuoCS/OpenOmni.svg?style=social&label=Star) <br> [**OpenOmni: A Fully Open-Source Omni Large Language Model with Real-time Self-Aware Emotional Speech Synthesis**](https://arxiv.org/pdf/2501.04561) <br> | arXiv | 2025-01-09 | [Github](https://github.com/RainBowLuoCS/OpenOmni) | - |
|  [**Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor**](https://arxiv.org/pdf/2412.17572) <br> | arXiv | 2024-12-23 | - | - |
| ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br> [**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**](https://arxiv.org/pdf/2501.01957) <br> | arXiv | 2024-12-20 | [Github](https://github.com/VITA-MLLM/VITA) | - |
| ![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star) <br> [**Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition**](https://arxiv.org/pdf/2412.09501) <br> | arXiv | 2024-12-12 | [Github](https://github.com/dvlab-research/Lyra) | - |
| ![Star](https://img.shields.io/github/stars/shansongliu/MuMu-LLaMA.svg?style=social&label=Star) <br> [**MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models**](https://arxiv.org/pdf/2412.06660) <br> | arXiv | 2024-12-09 | [Github](https://github.com/shansongliu/MuMu-LLaMA) | - |
| [**LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**](https://arxiv.org/pdf/2411.19772) <br> | arXiv | 2024-11-29 | -| - |
| [**Spider: Any-to-Many Multimodal LLM**](https://arxiv.org/pdf/2411.09439) <br> | arXiv | 2024-11-14 | - | - |
| [**Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model**](https://arxiv.org/pdf/2411.05903) <br> | arXiv | 2024-11-08 |- | - |
| ![Star](https://img.shields.io/github/stars/CAD-MLLM/CAD-MLLM.svg?style=social&label=Star) <br> [**CAD-MLLM: Unifying Multimodality-ConditionedCAD Generation With MLLM**](https://arxiv.org/pdf/2411.04954) <br> | arXiv | 2024-11-07 | [Github](https://github.com/CAD-MLLM/CAD-MLLM) | - |
| ![Star](https://img.shields.io/github/stars/JiazuoYu/PathWeave.svg?style=social&label=Star) <br> [**LLMs Can Evolve Continually on Modality for X-Modal Reasoning**](https://arxiv.org/pdf/2410.20178) <br> | NeurIPS | 2024-10-26 | [Github](https://github.com/JiazuoYu/PathWeave) | - |
|  [**OMCAT Omni Context Aware Transformer**](https://arxiv.org/pdf/2410.12109) <br> | arXiv | 2024-10-15 | - | - |
| ![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni2.svg?style=social&label=Star) <br> [**Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities**](https://arxiv.org/pdf/2410.11190) <br> | NeurIPS | 2024-10-15 | [Github](https://github.com/gpt-omni/mini-omni2) | - |
|  [**Aligned Better, Listen Better For Audio-Visual Large Language Models**](https://openreview.net/pdf?id=1SYUKPeM12) <br> | ICLR | 2024-10-13 | - | - |
| ![Star](https://img.shields.io/github/stars/westlake-baichuan-mllm/bc-omni.svg?style=social&label=Star) <br> [**Baichuan-Omni Technical Report**](https://arxiv.org/pdf/2410.08565) <br> | arXiv | 2024-10-11 | [Github](https://github.com/westlake-baichuan-mllm/bc-omni) | - |
| [**Efficient Generative Multimodal Integration (EGMI): Enabling Audio Generation from Text-Image Pairs through Alignment with Large Language Models**](https://openreview.net/pdf?id=5fWY2ZlsKj) <br> | NeurIPS<br>Workshop | 2024-10-11 | - | - |
|   [**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**](https://arxiv.org/pdf/2409.18042) <br> | arXiv | 2024-09-26 | -| - |
|  [**Large Language Models Are Strong Audio-Visual Speech Recognition Learners**](https://arxiv.org/abs/2409.12319) <br> | ICASSP | 2024-09-18 | -| - |
| ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br> [**VITA: Towards Open-Source Interactive Omni Multimodal LLM**](https://arxiv.org/pdf/2408.05211) <br> | arXiv | 2024-08-05 | [Github](https://github.com/VITA-MLLM/VITA) | - |
| ![Star](https://img.shields.io/github/stars/lzw-lzw/UnifiedMLLM.svg?style=social&label=Star) <br> [**UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model**](https://arxiv.org/pdf/2408.02503) <br> | arXiv | 2024-08-03 | [Github](https://github.com/lzw-lzw/UnifiedMLLM) | - |
| [**InternOmni: Extending InternVL with Audio Modality**](https://internvl.github.io/blog/2024-07-27-InternOmni/) <br> | blog | 2024-07-31 | - | - |
| ![Star](https://img.shields.io/github/stars/schowdhury671/meerkat.svg?style=social&label=Star) <br> [**Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time**](https://arxiv.org/pdf/2407.01851) <br> | arXiv | 2024-07-01 | [Github](https://github.com/schowdhury671/meerkat) | - |
| ![Star](https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&label=Star) <br> [**video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models**](https://arxiv.org/pdf/2406.15704) <br> | arXiv | 2024-06-22 | [Github](https://github.com/bytedance/SALMONN/) | - |
| ![Star](https://img.shields.io/github/stars/ZebangCheng/Emotion-LLaMA.svg?style=social&label=Star) <br> [**Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning**](https://arxiv.org/pdf/2406.11161) <br> | NIPS  | 2024-06-17 | [Github](https://github.com/ZebangCheng/Emotion-LLaMA) | [Demo](https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA) |
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star) <br> [**VideoLLaMA 2 Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**](https://arxiv.org/pdf/2406.07476) <br> | arXiv  | 2024-06-11 | [Github](https://github.com/DAMO-NLP-SG/VideoLLaMA2) | [Demo](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2-AV) |
| ![Star](https://img.shields.io/github/stars/scofield7419/MUIE-REAMO.svg?style=social&label=Star) <br> [**Recognizing Everything from All Modalities at Once: Grounded Multimodal Universal Information Extraction**](https://aclanthology.org/2024.findings-acl.863.pdf) <br> | ACL  | 2024-06-06 | [Github](https://github.com/scofield7419/MUIE-REAMO) |-|
| [**X-VILA: Cross-Modality Alignment for Large Language Model**](https://arxiv.org/pdf/2405.19335) <br> | arXiv  | 2024-05-29 | - |- |
| ![Star](https://img.shields.io/github/stars/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.svg?style=social&label=Star) <br> [**Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts**](https://arxiv.org/pdf/2405.19335) <br> | TPAMI  | 2024-05-18 | [Github](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs) | Local<br>Demo |
| ![Star](https://img.shields.io/github/stars/Rainlt/QaP.svg?style=social&label=Star) <br> [**Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf) <br> | CVPR  | 2024-05-06 | [Github](https://github.com/Rainlt/QaP/) |-|
| ![Star](https://img.shields.io/github/stars/DCDmllm/WorldGPT.svg?style=social&label=Star) <br> [**WorldGPT: Empowering LLM as Multimodal World Model**](https://arxiv.org/pdf/2404.18202) <br> | AAAI  | 2024-04-28 | [Github](https://github.com/DCDmllm/WorldGPT) |-|
|  [**Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding**](https://arxiv.org/pdf/2403.16276) <br> | arXiv  | 2024-03-24 | - |-|
| ![Star](https://img.shields.io/github/stars/rikeilong/Bay-CAT.svg?style=social&label=Star) <br> [**CAT : Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios**](https://arxiv.org/pdf/2403.04640) <br> | ECCV  | 2024-03-04 | [Github](https://github.com/rikeilong/Bay-CAT) |-|
| ![Star](https://img.shields.io/github/stars/THUNLP-MT/ModelCompose.svg?style=social&label=Star) <br> [**Model Composition for Multimodal Large Language Models**](https://arxiv.org/pdf/2402.12750) <br> | ACL  | 2024-02-18 | [Github](https://github.com/THUNLP-MT/ModelCompose) |-|
| ![Star](https://img.shields.io/github/stars/lzw-lzw/GroundingGPT.svg?style=social&label=Star) <br> [**GroundingGPT: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/pdf/2401.06071) <br> | ACL  | 2024-02-11 | [Github](https://github.com/lzw-lzw/GroundingGPT) |-|
| ![Star](https://img.shields.io/github/stars/Yui010206/CREMA.svg?style=social&label=Star) <br> [**CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion**](https://arxiv.org/pdf/2402.05889) <br> | ICLR  | 2024-02-08 | [Github](https://github.com/Yui010206/CREMA) |-|
| ![Star](https://img.shields.io/github/stars/UMass-Foundation-Model/MultiPLY.svg?style=social&label=Star) <br> [**MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World**](https://arxiv.org/pdf/2401.08577) <br> | CVPR  | 2024-01-16 | [Github](https://github.com/UMass-Foundation-Model/MultiPLY) |-|
| ![Star](https://img.shields.io/github/stars/xinke-wang/ModaVerse.svg?style=social&label=Star) <br> [**ModaVerse: Efficiently Transforming Modalities with LLMs**](https://arxiv.org/pdf/2401.06395) <br> | CVPR  | 2024-01-12 | [Github](https://github.com/xinke-wang/ModaVerse) |-|
| ![Star](https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&label=Star) <br> [**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Actions**](https://arxiv.org/pdf/2312.17172) <br> | CVPR  | 2023-12-28 | [Github](https://github.com/allenai/unified-io-2) |-|
| ![Star](https://img.shields.io/github/stars/OpenM3D/M3DBench.svg?style=social&label=Star) <br> [**M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07509.pdf) <br> | ECCV  | 2023-12-17 | [Github](https://github.com/OpenM3D/M3DBench) |-|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM.svg?style=social&label=Star) <br> [**DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving**](https://arxiv.org/pdf/2312.09245) <br> | arXiv  | 2023-12-14 | [Github](https://github.com/OpenGVLab/DriveMLM) |-|
|  [**Audio-Visual LLM for Video Understanding**](https://arxiv.org/pdf/2312.06720) <br> | arXiv  | 2023-12-11 |- |-|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning**](https://arxiv.org/pdf/2311.18799) <br> | ECCV  | 2023-11-30 | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip) |-|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LAMM.svg?style=social&label=Star) <br> [**Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE**](https://arxiv.org/pdf/2311.02684) <br> | ICLR  | 2023-11-05 | [Github](https://github.com/OpenGVLab/LAMM) |-|
| ![Star](https://img.shields.io/github/stars/BriansIDP/AudioVisualLLM.svg?style=social&label=Star) <br> [**Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models**](https://arxiv.org/pdf/2310.05863) <br> | arXiv  | 2023-10-09 | [Github](https://github.com/BriansIDP/AudioVisualLLM) |[Demo](https://881c5a6a6db84b1a2f.gradio.live/)|
| ![Star](https://img.shields.io/github/stars/kyegomez/AnyMAL.svg?style=social&label=Star) <br> [**AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model**](https://aclanthology.org/2024.emnlp-industry.98.pdf) <br> | EMNLP<br>industry  | 2023-09-27 | [Github](https://github.com/kyegomez/AnyMAL) |-|
| ![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star) <br> [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519) <br> | ICML  | 2023-09-11 | [Github](https://github.com/NExT-GPT/NExT-GPT) |-|
| ![Star](https://img.shields.io/github/stars/magic-research/bubogpt.svg?style=social&label=Star) <br> [**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**](https://arxiv.org/pdf/2309.05519) <br> | arXiv  | 2023-07-17 | [Github](https://github.com/magic-research/bubogpt) |-|
| ![Star](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM.svg?style=social&label=Star) <br> [**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://arxiv.org/pdf/2306.09093) <br> | arXiv  | 2023-06-15 | [Github](https://github.com/lyuchenyang/Macaw-LLM) |-|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LAMM.svg?style=social&label=Star) <br> [**LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark**](https://proceedings.neurips.cc/paper_files/paper/2023/file/548a41b9cac6f50dccf7e63e9e1b1b9b-Paper-Datasets_and_Benchmarks.pdf) <br> | NerIPS  | 2023-06-13 | [Github](https://github.com/OpenGVLab/LAMM) |-|
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://aclanthology.org/2023.emnlp-demo.49.pdf) <br> | EMNLP<br>demo  | 2023-06-05 | [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) |-|
| ![Star](https://img.shields.io/github/stars/joez17/ChatBridge.svg?style=social&label=Star) <br> [**ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst**](https://arxiv.org/pdf/2305.16103) <br> | arXiv | 2023-05-25 | [Github](https://github.com/joez17/ChatBridge) |-|
| ![Star](https://img.shields.io/github/stars/phellonchen/X-LLM.svg?style=social&label=Star) <br> [**X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages**](https://arxiv.org/pdf/2305.04160) <br> | arXiv | 2023-05-07 | [Github](https://github.com/phellonchen/X-LLM) |-|
| ![Star](https://img.shields.io/github/stars/mshukor/eP-ALM.svg?style=social&label=Star) <br> [**eP-ALM: Efficient Perceptual Augmentation of Language Models**](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf) <br> | ICCV | 2023-03-20 | [Github](https://github.com/mshukor/eP-ALM) |-|
<!-- | ![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star) <br> [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://arxiv.org/pdf/2311.18775) <br> | CVPR  | 2025-02-07 | [Github](https://github.com/microsoft/i-Code/tree/main/CoDi-2) |-| -->